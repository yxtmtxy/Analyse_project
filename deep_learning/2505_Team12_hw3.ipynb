{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#TRAIN.py"
      ],
      "metadata": {
        "id": "tJDevTECuaVW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvHSl3BouQvC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from torch.utils.data import ConcatDataset, DataLoader, Subset\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torchvision.datasets import DatasetFolder\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, norm_layer=None):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "\n",
        "        # First convolution layer\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = norm_layer(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Second convolution layer\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = norm_layer(out_channels)\n",
        "\n",
        "        # Shortcut connection\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                norm_layer(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.shortcut(x)\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += identity  # Add shortcut\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, num_classes=14):\n",
        "        super(Classifier, self).__init__()\n",
        "\n",
        "        # Weight decay for L2 regularization (used in optimizer)\n",
        "        self.weight_decay = 0.0001\n",
        "\n",
        "        # Block 1\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(p=0.2)\n",
        "        )\n",
        "\n",
        "        # Block 2\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(p=0.3)\n",
        "        )\n",
        "\n",
        "        self.block3 = nn.Sequential(\n",
        "            ResidualBlock(128, 256, stride=2),\n",
        "            ResidualBlock(256, 256)\n",
        "        )\n",
        "\n",
        "        # Global Average Pooling\n",
        "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # Fully Connected Layer\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.gap(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class EarlyStopping:\n",
        "        def __init__(self, patience=7, verbose=False, delta=0):\n",
        "            self.patience = patience\n",
        "            self.verbose = verbose\n",
        "            self.counter = 0\n",
        "            self.best_score = None\n",
        "            self.early_stop = False\n",
        "            self.val_loss_min = float('inf')\n",
        "            self.delta = delta\n",
        "\n",
        "        def __call__(self, val_loss, model):\n",
        "            score = -val_loss\n",
        "\n",
        "            if self.best_score is None:\n",
        "                self.best_score = score\n",
        "                self.save_checkpoint(val_loss, model)\n",
        "            elif score < self.best_score + self.delta:\n",
        "                self.counter += 1\n",
        "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "                if self.counter >= self.patience:\n",
        "                    self.early_stop = True\n",
        "            else:\n",
        "                self.best_score = score\n",
        "                self.save_checkpoint(val_loss, model)\n",
        "                self.counter = 0\n",
        "\n",
        "        def save_checkpoint(self, val_loss, model):\n",
        "            '''Saves model when validation loss decrease.'''\n",
        "            if self.verbose:\n",
        "                print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "            torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "            self.val_loss_min = val_loss\n",
        "\n",
        "def get_pseudo_labels(dataset, model, threshold=0.7):\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        model.eval()\n",
        "        softmax = nn.Softmax(dim=-1)\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "        pseudo_labeled_data = []\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(dataloader, desc=\"Generating Pseudo Labels\"):\n",
        "                img, _ = batch\n",
        "                img = img.to(device)\n",
        "\n",
        "                logits = model(img)\n",
        "                probs = softmax(logits)\n",
        "\n",
        "                max_probs, pseudo_labels = torch.max(probs, dim=-1)\n",
        "\n",
        "\n",
        "                mask = max_probs >= threshold\n",
        "                pseudo_labeled_data.extend([\n",
        "                    (img[i].cpu(), pseudo_labels[i].item())\n",
        "                    for i in range(len(img)) if mask[i]\n",
        "                ])\n",
        "\n",
        "        model.train()\n",
        "        return pseudo_labeled_data\n",
        "\n",
        "def convert_to_rgb(image):\n",
        "        return image.convert('RGB')\n",
        "\n",
        "def image_loader(path):\n",
        "        return Image.open(path)\n",
        "\n",
        "'''\n",
        "# calculate mean and std\n",
        "#Mean: tensor([0.4197, 0.3729, 0.2556])\n",
        "#Std: tensor([0.2478, 0.2095, 0.1908])\n",
        "mean = torch.zeros(3)\n",
        "std = torch.zeros(3)\n",
        "total_images = 0\n",
        "\n",
        "for images, _ in tqdm(train_loader, desc=\"Calculating Mean and Std\"):\n",
        "    batch_samples = images.size(0)\n",
        "    images = images.view(batch_samples, 3, -1)\n",
        "\n",
        "    mean += images.mean(dim=2).sum(dim=0)\n",
        "    std += images.std(dim=2).sum(dim=0)\n",
        "    total_images += batch_samples\n",
        "\n",
        "mean /= total_images\n",
        "std /= total_images\n",
        "\n",
        "print(f\"Mean: {mean}\")\n",
        "print(f\"Std: {std}\")\n",
        "'''\n",
        "'''\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        # input image size: [3, 128, 128]\n",
        "        self.cnn_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, 1, 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),\n",
        "\n",
        "            nn.Conv2d(64, 128, 3, 1, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),\n",
        "\n",
        "            nn.Conv2d(128, 256, 3, 1, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(4, 4, 0),\n",
        "        )\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(256 * 8 * 8, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, NUM_CLASSES)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn_layers(x)\n",
        "        x = x.flatten(1)\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "'''\n",
        "if __name__ == '__main__':\n",
        "    folder = 'dataset'\n",
        "    NUM_CLASSES = 14\n",
        "    #Normalization of Image Data\n",
        "    mean = [0.4197, 0.3729, 0.2556]\n",
        "    std = [0.2478, 0.2095, 0.1908]\n",
        "\n",
        "    train_tfm = transforms.Compose([\n",
        "        transforms.Resize((128, 128)),\n",
        "        transforms.Lambda(convert_to_rgb),\n",
        "        #  Data Augmentation\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomAffine(degrees=0, translate=(0.12, 0.12)),\n",
        "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "        transforms.RandomResizedCrop(128, scale=(0.9, 1.0)),\n",
        "        transforms.RandomAffine(degrees=0, shear=(-10, 10)),\n",
        "\n",
        "        transforms.ToTensor(),\n",
        "        #Normalize\n",
        "        transforms.Normalize(mean=mean, std=std),\n",
        "    ])\n",
        "\n",
        "    test_tfm = transforms.Compose([\n",
        "        transforms.Resize((128, 128)),\n",
        "        transforms.Lambda(convert_to_rgb),\n",
        "        transforms.ToTensor(),\n",
        "        #Normalize\n",
        "        transforms.Normalize(mean=mean, std=std),\n",
        "    ])\n",
        "\n",
        "    train_set = DatasetFolder(folder + \"/train/labeled\", loader=image_loader, extensions=\"jpg\", transform=train_tfm)\n",
        "    valid_set = DatasetFolder(folder + \"/val\", loader=image_loader, extensions=\"jpg\", transform=test_tfm)\n",
        "    unlabeled_set = DatasetFolder(folder + \"/train/unlabeled\", loader=image_loader, extensions=\"jpg\", transform=train_tfm)\n",
        "    test_set = DatasetFolder(folder + \"/test\", loader=image_loader, extensions=\"jpg\", transform=test_tfm)\n",
        "\n",
        "    batch_size = 128\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "    #伪标签\n",
        "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = Classifier().to(device)\n",
        "    model.device = device\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay= 0.0001)\n",
        "\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, min_lr=0.0005, verbose=True)\n",
        "    early_stopping = EarlyStopping(patience=40, verbose=True)\n",
        "    n_epochs = 70\n",
        "\n",
        "    # Whether to do semi-supervised learning.\n",
        "    do_semi = True\n",
        "    train_losses, train_accs, valid_losses, valid_accs = [], [], [], []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        if do_semi:\n",
        "\n",
        "            pseudo_set = get_pseudo_labels(unlabeled_set, model)\n",
        "            concat_dataset = ConcatDataset([train_set, pseudo_set])\n",
        "            train_loader = DataLoader(concat_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "        # ---------- train ----------\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch in tqdm(train_loader):\n",
        "            imgs, labels = batch\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            logits = model(imgs)\n",
        "            loss = criterion(logits, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = logits.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_acc = correct / total\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "\n",
        "\n",
        "        # ---------- Validation ----------\n",
        "        model.eval()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(valid_loader):\n",
        "                imgs, labels = batch\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "                logits = model(imgs)\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "                running_loss += loss.item()\n",
        "                _, predicted = logits.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        valid_loss = running_loss / len(valid_loader)\n",
        "        valid_acc = correct / total\n",
        "        valid_losses.append(valid_loss)\n",
        "        valid_accs.append(valid_acc)\n",
        "\n",
        "        scheduler.step(valid_loss)\n",
        "        early_stopping(valid_loss, model)\n",
        "\n",
        "        print(f\"[Epoch {epoch + 1}/{n_epochs}] Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
        "              f\"Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_acc:.4f}\")\n",
        "\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(valid_losses, label='Valid Loss')\n",
        "    plt.title('Loss Curve')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accs, label='Train Accuracy')\n",
        "    plt.plot(valid_accs, label='Valid Accuracy')\n",
        "    plt.title('Accuracy Curve')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.savefig('learning.jpg', dpi=300, bbox_inches='tight')\n",
        "    print(\"Learning curves saved as 'learning.jpg'.\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    model_save_path = 'trained_model.pth'\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "    print(f\"Trained model saved to '{model_save_path}'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TEST.py"
      ],
      "metadata": {
        "id": "qTNojXGWuesX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from torch.utils.data import ConcatDataset, DataLoader, Subset\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torchvision.datasets import DatasetFolder\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, norm_layer=None):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "\n",
        "        # First convolution layer\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = norm_layer(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Second convolution layer\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = norm_layer(out_channels)\n",
        "\n",
        "        # Shortcut connection\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                norm_layer(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.shortcut(x)\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += identity  # Add shortcut\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, num_classes=14):\n",
        "        super(Classifier, self).__init__()\n",
        "\n",
        "        # Weight decay for L2 regularization (used in optimizer)\n",
        "        self.weight_decay = 0.0001\n",
        "\n",
        "        # Block 1\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(p=0.2)\n",
        "        )\n",
        "\n",
        "        # Block 2\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(p=0.3)\n",
        "        )\n",
        "\n",
        "        self.block3 = nn.Sequential(\n",
        "            ResidualBlock(128, 256, stride=2),\n",
        "            ResidualBlock(256, 256)\n",
        "        )\n",
        "\n",
        "        # Global Average Pooling\n",
        "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # Fully Connected Layer\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.gap(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class EarlyStopping:\n",
        "        def __init__(self, patience=7, verbose=False, delta=0):\n",
        "            self.patience = patience\n",
        "            self.verbose = verbose\n",
        "            self.counter = 0\n",
        "            self.best_score = None\n",
        "            self.early_stop = False\n",
        "            self.val_loss_min = float('inf')\n",
        "            self.delta = delta\n",
        "\n",
        "        def __call__(self, val_loss, model):\n",
        "            score = -val_loss\n",
        "\n",
        "            if self.best_score is None:\n",
        "                self.best_score = score\n",
        "                self.save_checkpoint(val_loss, model)\n",
        "            elif score < self.best_score + self.delta:\n",
        "                self.counter += 1\n",
        "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "                if self.counter >= self.patience:\n",
        "                    self.early_stop = True\n",
        "            else:\n",
        "                self.best_score = score\n",
        "                self.save_checkpoint(val_loss, model)\n",
        "                self.counter = 0\n",
        "\n",
        "        def save_checkpoint(self, val_loss, model):\n",
        "            '''Saves model when validation loss decrease.'''\n",
        "            if self.verbose:\n",
        "                print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "            torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "            self.val_loss_min = val_loss\n",
        "\n",
        "def get_pseudo_labels(dataset, model, threshold=0.7):\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        model.eval()\n",
        "        softmax = nn.Softmax(dim=-1)\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "        pseudo_labeled_data = []\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(dataloader, desc=\"Generating Pseudo Labels\"):\n",
        "                img, _ = batch\n",
        "                img = img.to(device)\n",
        "\n",
        "                logits = model(img)\n",
        "                probs = softmax(logits)\n",
        "\n",
        "\n",
        "                max_probs, pseudo_labels = torch.max(probs, dim=-1)\n",
        "\n",
        "\n",
        "                mask = max_probs >= threshold\n",
        "                pseudo_labeled_data.extend([\n",
        "                    (img[i].cpu(), pseudo_labels[i].item())\n",
        "                    for i in range(len(img)) if mask[i]\n",
        "                ])\n",
        "\n",
        "        model.train()\n",
        "        return pseudo_labeled_data\n",
        "\n",
        "def convert_to_rgb(image):\n",
        "        return image.convert('RGB')\n",
        "\n",
        "def image_loader(path):\n",
        "        return Image.open(path)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    folder = 'dataset'\n",
        "    NUM_CLASSES = 14\n",
        "    # Normalization of Image Data\n",
        "    mean = [0.4197, 0.3729, 0.2556]\n",
        "    std = [0.2478, 0.2095, 0.1908]\n",
        "\n",
        "    test_tfm = transforms.Compose([\n",
        "        transforms.Resize((128, 128)),\n",
        "        transforms.Lambda(convert_to_rgb),\n",
        "        transforms.ToTensor(),\n",
        "        # Normalize\n",
        "        transforms.Normalize(mean=mean, std=std),\n",
        "    ])\n",
        "    test_set = DatasetFolder(folder + \"/test\", loader=image_loader, extensions=\"jpg\", transform=test_tfm)\n",
        "    batch_size = 128\n",
        "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    num_images = len(test_set)\n",
        "    print(f\"Number of images loaded: {num_images}\")\n",
        "\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model_path = \"trained_model.pth\"\n",
        "    model = Classifier(num_classes=14)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    for batch in tqdm(test_loader):\n",
        "        imgs, labels = batch\n",
        "        imgs = imgs.to(device)\n",
        "        with torch.no_grad():\n",
        "            logits = model(imgs)\n",
        "\n",
        "        predictions.extend(logits.argmax(dim=-1).cpu().numpy().tolist())\n",
        "\n",
        "    # Save predictions into the file.\n",
        "    with open(\"predict.csv\", \"w\") as f:\n",
        "\n",
        "        # The first row must be \"Id, Category\"\n",
        "        f.write(\"Id,Category\\n\")\n",
        "\n",
        "        # For the rest of the rows, each image id corresponds to a predicted class.\n",
        "        for i, pred in enumerate(predictions):\n",
        "            f.write(f\"{i},{pred}\\n\")\n"
      ],
      "metadata": {
        "id": "It7L3EQ3uXAX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}